{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\MY\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "file_docs = []\n",
    "\n",
    "with open ('blog1.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file_docs.append(line)\n",
    "\n",
    "print(\"Number of documents:\",len(file_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open file and tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in file_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize words and create dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3pkd\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'a': 1, 'across': 2, 'bit': 3, 'come': 4, 'different': 5, 'from': 6, 'is': 7, 'my': 8, 'other': 9, 'placement': 10, 'stories': 11, 'story': 12, 'that': 13, 'the': 14, 'will': 15, 'you': 16, 'began': 17, 'break': 18, 'during': 19, 'for': 20, 'i': 21, 'internship': 22, 'preparation': 23, 'semester': 24, 'session': 25, 'two-month': 26, ',': 27, '10': 28, 'able': 29, 'an': 30, 'and': 31, 'any': 32, 'appeared': 33, 'but': 34, 'clear': 35, 'companies': 36, 'company': 37, 'due': 38, 'even': 39, 'favoring': 40, 'first': 41, 'flaws': 42, 'get': 43, 'hope': 44, 'in': 45, 'intern': 46, 'luck': 47, 'many': 48, 'me': 49, 'not': 50, 'of': 51, 'round': 52, 'strategy': 53, 't': 54, 'to': 55, 'wasn': 56, 'with': 57, 'written': 58, '’': 59, 'all': 60, 'combined': 61, 'correct': 62, 'factors': 63, 'goal': 64, 'higher': 65, 'led': 66, 'path': 67, 'pursuing': 68, 'should': 69, 'stick': 70, 'studies': 71, 'these': 72, 'think': 73, 'this': 74, '3rd': 75, 'continued': 76, 'never': 77, 'placements': 78, 'planned': 79, 'preparing': 80, 'respective': 81, 'rest': 82, 'sit': 83, 'so': 84, 'year': 85, 'act': 86, 'actually': 87, 'advised': 88, 'around': 89, 'as': 90, 'booster': 91, 'close': 92, 'confidence': 93, 'end': 94, 'finally': 95, 'friend': 96, 'it': 97, 'krishan': 98, 'made': 99, 'march': 100, 'mind': 101, 'mine': 102, 'month': 103, 'pandey': 104, 'then': 105, 'up': 106, 'vijay': 107, 'was': 108, 'would': 109, '2.5': 110, 'at': 111, 'by': 112, 'got': 113, 'how': 114, 'months': 115, 'now': 116, 'phonepe': 117, 'placed': 118, 'tell': 119, 'wholeheartedly': 120, 'coding': 121, 'drive': 122, 'experience': 123, 'failing': 124, 'needed': 125, 'on': 126, 'skills': 127, 'work': 128, 'book': 129, 'coded': 130, 'committed': 131, 'gfg': 132, 'mistake': 133, 'negligible': 134, 'number': 135, 'questions': 136, 'reading': 137, 'started': 138, 'theory': 139, 'whereas': 140, 'bag': 141, 'help': 142, 'isn': 143, 'might': 144, 'or': 145, 'perhaps': 146, 'sure-shot': 147, 'way': 148, 'after': 149, 'face': 150, 'just': 151, 'mentally': 152, 'prepared': 153, 'quit': 154, 'rejection': 155, 'sitting': 156, 'do': 157, 'down': 158, 'happen': 159, 'have': 160, 'keep': 161, 'please': 162, 'revising': 163, 'some': 164, 'studied': 165, 'thoroughly': 166, 'those': 167, 'two': 168, 'whatever': 169}\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(gen_docs)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "from gensim import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf is calculated by multiplying a local component (TF) with a global component (IDF) and optionally normalizing the result to unit length. Term frequency is how often the word shows up in the document and inverse document frequency scales the value by how rare the word is in the corpus. In simple terms, words that occur more frequently across the documents get smaller weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 0.11], ['across', 0.31], ['bit', 0.31], ['come', 0.31], ['different', 0.31], ['from', 0.31], ['is', 0.31], ['my', 0.04], ['other', 0.31], ['placement', 0.17], ['stories', 0.31], ['story', 0.31], ['that', 0.11], ['the', 0.05], ['will', 0.22], ['you', 0.14]]\n",
      "[['my', 0.05], ['the', 0.15], ['began', 0.45], ['break', 0.33], ['during', 0.2], ['for', 0.13], ['i', 0.05], ['internship', 0.16], ['preparation', 0.25], ['semester', 0.45], ['session', 0.33], ['two-month', 0.45]]\n",
      "[['a', 0.07], ['my', 0.02], ['the', 0.1], ['i', 0.05], ['preparation', 0.11], [',', 0.07], ['10', 0.14], ['able', 0.2], ['an', 0.14], ['and', 0.06], ['any', 0.2], ['appeared', 0.2], ['but', 0.11], ['clear', 0.14], ['companies', 0.11], ['company', 0.2], ['due', 0.2], ['even', 0.2], ['favoring', 0.2], ['first', 0.29], ['flaws', 0.2], ['get', 0.2], ['hope', 0.2], ['in', 0.22], ['intern', 0.2], ['luck', 0.2], ['many', 0.2], ['me', 0.09], ['not', 0.14], ['of', 0.09], ['round', 0.29], ['strategy', 0.14], ['t', 0.09], ['to', 0.13], ['wasn', 0.11], ['with', 0.14], ['written', 0.2], ['’', 0.09]]\n",
      "[['my', 0.03], ['that', 0.09], ['the', 0.04], ['for', 0.07], ['i', 0.03], ['and', 0.07], ['me', 0.23], ['of', 0.06], ['t', 0.11], ['to', 0.11], ['wasn', 0.14], ['’', 0.11], ['all', 0.25], ['combined', 0.25], ['correct', 0.25], ['factors', 0.25], ['goal', 0.18], ['higher', 0.25], ['led', 0.18], ['path', 0.25], ['pursuing', 0.25], ['should', 0.25], ['stick', 0.25], ['studies', 0.25], ['these', 0.25], ['think', 0.25], ['this', 0.14]]\n",
      "[['my', 0.04], ['the', 0.05], ['for', 0.26], ['i', 0.07], [',', 0.11], ['and', 0.09], ['of', 0.07], ['to', 0.07], ['goal', 0.23], ['3rd', 0.23], ['continued', 0.31], ['never', 0.31], ['placements', 0.31], ['planned', 0.31], ['preparing', 0.23], ['respective', 0.31], ['rest', 0.31], ['sit', 0.23], ['so', 0.23], ['year', 0.23]]\n",
      "[['a', 0.13], ['my', 0.02], ['placement', 0.1], ['the', 0.06], ['for', 0.1], ['i', 0.02], [',', 0.2], ['and', 0.05], ['in', 0.1], ['me', 0.16], ['of', 0.12], ['to', 0.04], ['3rd', 0.13], ['sit', 0.13], ['year', 0.13], ['act', 0.19], ['actually', 0.19], ['advised', 0.19], ['around', 0.19], ['as', 0.21], ['booster', 0.19], ['close', 0.19], ['confidence', 0.19], ['end', 0.19], ['finally', 0.19], ['friend', 0.19], ['it', 0.21], ['krishan', 0.19], ['made', 0.13], ['march', 0.19], ['mind', 0.19], ['mine', 0.19], ['month', 0.19], ['pandey', 0.19], ['then', 0.19], ['up', 0.19], ['vijay', 0.19], ['was', 0.1], ['would', 0.19]]\n",
      "[['will', 0.2], ['you', 0.13], ['for', 0.08], ['i', 0.07], ['preparing', 0.2], ['so', 0.2], ['2.5', 0.28], ['at', 0.28], ['by', 0.28], ['got', 0.28], ['how', 0.28], ['months', 0.2], ['now', 0.28], ['phonepe', 0.28], ['placed', 0.28], ['tell', 0.28], ['wholeheartedly', 0.28]]\n",
      "[['my', 0.06], ['that', 0.09], ['the', 0.09], ['during', 0.12], ['i', 0.06], ['internship', 0.09], ['clear', 0.39], ['companies', 0.15], ['first', 0.19], ['of', 0.06], ['round', 0.19], ['to', 0.12], ['as', 0.15], ['it', 0.15], ['made', 0.19], ['was', 0.15], ['coding', 0.27], ['drive', 0.27], ['experience', 0.27], ['failing', 0.27], ['needed', 0.27], ['on', 0.27], ['skills', 0.27], ['work', 0.27]]\n",
      "[['a', 0.19], ['my', 0.03], ['that', 0.19], ['the', 0.04], ['during', 0.12], ['i', 0.09], ['internship', 0.09], ['preparation', 0.15], ['of', 0.06], ['as', 0.15], ['was', 0.15], ['book', 0.26], ['coded', 0.26], ['committed', 0.26], ['gfg', 0.26], ['mistake', 0.26], ['negligible', 0.26], ['number', 0.26], ['questions', 0.26], ['reading', 0.26], ['started', 0.26], ['theory', 0.26], ['whereas', 0.26]]\n",
      "[['a', 0.11], ['placement', 0.17], ['you', 0.14], ['internship', 0.11], ['an', 0.22], ['but', 0.17], ['strategy', 0.22], ['t', 0.14], ['to', 0.07], ['’', 0.14], ['this', 0.17], ['it', 0.17], ['bag', 0.31], ['help', 0.31], ['isn', 0.31], ['might', 0.22], ['or', 0.31], ['perhaps', 0.31], ['sure-shot', 0.31], ['way', 0.31]]\n",
      "[['my', 0.03], ['that', 0.1], ['during', 0.13], ['for', 0.08], ['i', 0.03], ['internship', 0.1], ['session', 0.21], [',', 0.1], ['10', 0.21], ['and', 0.08], ['companies', 0.16], ['me', 0.13], ['t', 0.13], ['to', 0.13], ['wasn', 0.16], ['’', 0.13], ['led', 0.21], ['after', 0.29], ['face', 0.29], ['just', 0.29], ['mentally', 0.29], ['prepared', 0.29], ['quit', 0.29], ['rejection', 0.29], ['sitting', 0.29]]\n",
      "[['you', 0.21], ['break', 0.17], [',', 0.08], ['and', 0.07], ['but', 0.13], ['in', 0.13], ['not', 0.17], ['of', 0.05], ['with', 0.17], ['this', 0.13], ['months', 0.17], ['might', 0.17], ['do', 0.24], ['down', 0.24], ['happen', 0.24], ['have', 0.24], ['keep', 0.24], ['please', 0.24], ['revising', 0.24], ['some', 0.24], ['studied', 0.24], ['thoroughly', 0.24], ['those', 0.24], ['two', 0.24], ['whatever', 0.24]]\n"
     ]
    }
   ],
   "source": [
    "tf_idf = models.TfidfModel(corpus)\n",
    "for doc in tf[corpus]:\n",
    "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we are going to create similarity object. The main class is Similarity, which builds an index for a given set of documents.The Similarity class splits the index into several smaller sub-indexes, which are disk-based. Let's just create similarity object then you will understand how we can use it for comparing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = similarities.Similarity('',tf_idf[corpus],\n",
    "                                        num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Document where I have stored another text file as a blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 19\n"
     ]
    }
   ],
   "source": [
    "file2_docs = []\n",
    "\n",
    "with open ('blog2.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file2_docs.append(line)\n",
    "\n",
    "print(\"Number of documents:\",len(file2_docs))  \n",
    "for line in file2_docs:\n",
    "    query_doc_tf_idf = [w.lower() for w in word_tokenize(line)]\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc_tf_idf) #update an existing dictionary and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document similarities to query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Result: [0.11946748 0.06188476 0.05472439 0.07977556 0.12668498 0.32534105\n",
      " 0.02892251 0.10277206 0.1436688  0.09660573 0.08641316 0.01215397]\n"
     ]
    }
   ],
   "source": [
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "print('Comparing Result:', sims[query_doc_tf_idf]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2384144\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "print(sum_of_sims)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next below code is for calculating the average similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average similarity float: 0.10320120056470235\n",
      "Average similarity percentage: 10.320120056470234\n",
      "Average similarity rounded percentage: 10\n"
     ]
    }
   ],
   "source": [
    "percentage_of_similarity = round(float((sum_of_sims / len(file_docs)) * 100))\n",
    "print(f'Average similarity float: {float(sum_of_sims / len(file_docs))}')\n",
    "print(f'Average similarity percentage: {float(sum_of_sims / len(file_docs)) * 100}')\n",
    "print(f'Average similarity rounded percentage: {percentage_of_similarity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
